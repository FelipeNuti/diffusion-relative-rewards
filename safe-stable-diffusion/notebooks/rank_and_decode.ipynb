{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENT WARNING: images produced may be shocking or distressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipelineSafe\n",
    "from rrf_diffusion import ValueUnet, GradientRewardRegressor, GradientMatchingTrainer\n",
    "from diffusers.pipelines.stable_diffusion_safe import SafetyConfig\n",
    "from rrf_diffusion.models import cycle\n",
    "from rrf_diffusion.dataset import GradientMatchingDataset, batch_to_device\n",
    "from rrf_diffusion.utils import check_nan, plot2img, to_np\n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_orig = StableDiffusionPipelineSafe.from_pretrained(\n",
    "    \"AIML-TUDA/stable-diffusion-safe\", \n",
    "    torch_dtype=torch.float16, \n",
    "    # cache_dir=\"/scratch/shared/beegfs/<username>/huggingface_cache\",\n",
    "    # device_map=\"auto\",\n",
    "    safety_checker = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'batch_size': 256, 'lr': 0.0001, 'gradient_clipping': None, 'sample_freq': 1000, 'n_train_steps': 1000000, 'n_steps_per_epoch': 1000, 'train_frac': 0.9, 'dim': 32, 'seed': 3, 'debug': False, 'test_overfit': False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/work/<username>/safe_stable_diffusion_data/train\"\n",
    "load_file = \"merged.pt\"\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "expert_dataset = torch.load(os.path.join(base_path, f\"expert/{load_file}\"), map_location=torch.device(\"cpu\"))\n",
    "general_dataset = torch.load(os.path.join(base_path, f\"general/{load_file}\"), map_location=torch.device(\"cpu\"))\n",
    "print(\"Finished loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ValueUnet(args[\"dim\"], dim_mults=(1, 2, 4, 8), channels = 4, resnet_block_groups=8)\n",
    "gradient_matching = GradientRewardRegressor(model)\n",
    "\n",
    "# utils.report_parameters(model)\n",
    "\n",
    "savepath = \"/scratch/shared/beegfs/<username>/safe_stable_diffusion_logs/lr0.0001_dim32_seed3\"\n",
    "\n",
    "os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model = model.to(device = device)\n",
    "gradient_matching = gradient_matching.to(device = device)\n",
    "\n",
    "trainer = GradientMatchingTrainer(\n",
    "    gradient_matching, \n",
    "    expert_dataset, \n",
    "    general_dataset,\n",
    "    train_lr=args[\"lr\"],\n",
    "    gradient_clipping=args[\"gradient_clipping\"],\n",
    "    train_batch_size=args[\"batch_size\"],\n",
    "    sample_freq=args[\"sample_freq\"],\n",
    "    train_frac=args[\"train_frac\"],\n",
    "    test_overfit=args[\"test_overfit\"],\n",
    "    results_folder=savepath,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logdir = \"/scratch/shared/beegfs/<username>/safe_stable_diffusion_logs/lr0.0001_dim32_seed3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.load(140000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline_orig.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sampling(n = 1, batch = 32):\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    max_reward = None\n",
    "    min_reward = None\n",
    "\n",
    "    argmax_reward = None\n",
    "    argmin_reward = None\n",
    "\n",
    "    label_max = None\n",
    "    label_min = None\n",
    "\n",
    "    mixed_dataloader_eval = cycle(torch.utils.data.DataLoader(\n",
    "        trainer.dataset_eval, batch_size=batch, num_workers=0, shuffle=True, pin_memory=True\n",
    "    ))\n",
    "\n",
    "    all_outs = []   \n",
    "    all_labels = []\n",
    "\n",
    "    for _ in tqdm(range(n)):\n",
    "        batch = next(mixed_dataloader_eval)\n",
    "        batch = batch_to_device(batch)\n",
    "        x_t, t, _, _, labels = batch\n",
    "        out, _, N = trainer._get_preds(x_t, t)\n",
    "\n",
    "        out = to_np(out)\n",
    "        labels = to_np(labels).flatten()\n",
    "\n",
    "        batch_argmax = np.argmax(out)#[0]\n",
    "        batch_argmin = np.argmin(out)#[0]\n",
    "\n",
    "        batch_max = out[batch_argmax]\n",
    "        batch_min = out[batch_argmin]\n",
    "\n",
    "        print(t)\n",
    "\n",
    "        if max_reward is None or batch_max > max_reward:\n",
    "            max_reward = batch_max\n",
    "            argmax_reward = x_t[batch_argmax]\n",
    "            label_max = labels[batch_argmax]\n",
    "        \n",
    "        if min_reward is None or batch_min < min_reward:\n",
    "            min_reward = batch_min\n",
    "            argmin_reward = x_t[batch_argmin]\n",
    "            label_min = labels[batch_argmin]\n",
    "\n",
    "\n",
    "    print(\"Rewards:\", min_reward, max_reward)\n",
    "    print(\"Labels:\", label_min, label_max)\n",
    "    latent_max = argmax_reward.to(dtype = torch.float16, device=\"cuda:0\").unsqueeze(0).detach()\n",
    "    latent_min = argmin_reward.to(dtype = torch.float16, device=\"cuda:0\").unsqueeze(0).detach()\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "        decoded_max = pipeline_orig.decode_latents(latent_max)\n",
    "    image_max = pipeline.numpy_to_pil(decoded_max)[0]\n",
    "    display(image_max)\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "        decoded_min = pipeline_orig.decode_latents(latent_min)\n",
    "    image_min = pipeline.numpy_to_pil(decoded_min)[0]\n",
    "    display(image_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sampling(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safe_env_diffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
